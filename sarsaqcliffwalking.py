# -*- coding: utf-8 -*-
"""SarsaQCliffWalking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qe6xlGdnA3yP9OJaKwf0Ko4u4-Wx4EQ8

# Sutton and Barton RL Cliff Walking Problem Solution
"""

#import libraries
import numpy as np
import matplotlib.pyplot as plt

"""#### Cliff Environment Class"""

class CliffEnv(object):
  def __init__(self):
    self.start = (3,0)
    self.goal = (3, 11)
    self.reward_table = np.ones((4,12))*-1 # 4 rows, 12 columns
    self.reward_table[self.goal] = 0
    self.reward_table[3,1:11] = -100
    self.topEdge = 0
    self.leftEdge = 0
    self.rightEdge = 11
    self.downEdge = 3
  
  def move(self, position, action):
    X, Y = position[0], position[1]
    #checking whether each action is allowed and updating
    if action == 0 and X > self.topEdge:
      X -= 1
    elif action == 1 and Y > self.leftEdge:
      Y -= 1
    elif action == 2 and Y < self.rightEdge:
      Y += 1
    elif action == 3 and X < self.downEdge:
      X = X + 1
    return (X,Y)
  
  def reward(self, position):
    #returns rewards of position on grid passed in as param
    return int(self.reward_table[position[0]][position[1]])

"""#### Agent Class"""

class Agent(object):
  def __init__(self):
    self.start = (3,0)
    self.goal = (3,11)
    self.env = CliffEnv()
  
  def chooseAction(self, state, q_tab, epsilon):
    if np.random.random() < epsilon:
      #exploratory choice
      return np.random.randint(0,4)
    #greedy choice
    return np.argmax(q_tab[:, state])

"""#### SARSA Implementation"""

def SarsaPlay(agent, episodes = 500, gamma = 1, alpha = 0.1, epsilon = 0.1):
  #Initialize Q table for state and action values
  q_tab = np.zeros((4, 4 * 12))
  #list for all rewards during each run
  all_rewards = []
  #list for final path during alst episode
  path = []
  #counting steps of last traversal
  final_iter_steps = 0
  #loop through episodes of traversal
  for i in range(episodes):
    #initialize agent position at the start
    pos = agent.start
    #while loop condition variable
    finished = False
    #sum up reward in each episode
    sum_reward = 0
    #work out initial state of agent
    state = 12 * pos[0] + pos[1]
    #choose action using e-greedy policy
    action = agent.chooseAction(state, q_tab, epsilon)
    while finished == False:
      
      #move agent - call environment move method passing in agent coords and action number
      #print(f"Agent position: {pos}, about to do action: {action}")
      pos = agent.env.move(pos, action)
      #print(f"Agent new position: {pos}, done action: {action}")

      
      #get next state
      next_state = 12 * pos[0] + pos[1]

      #get reward
      reward = agent.env.reward(pos)
      sum_reward += reward
      
      #print(agent.env.reward_table)
      #finished = True

      #see if game ends - check value of reward retrieved from previous method call
      if reward == -100:
        #print("done - cliff")
        finished = True
      elif reward == 0:
        #print("done - end")
        finished = True

      #if finished, update with 0 next state val, else, normal q update and action select
      if finished == True:
        q_tab[action][state] = q_tab[action][state] + alpha * (reward + (gamma * 0.0) - q_tab[action][state])
        break
      else:
        #choose next action using policy and next state
        next_action = agent.chooseAction(next_state, q_tab, epsilon)

        #update q table
        next_state_val = q_tab[next_action][next_state]
        q_tab[action][state] = q_tab[action][state] + alpha * (reward + (gamma * next_state_val) - q_tab[action][state])

      #update state and action - both must be updated - difference between sarsa and q
      state = next_state
      action = next_action
      #record of last episode traversal path
      if i == episodes - 1:
        path.append(pos)
        final_iter_steps += 1
    all_rewards.append(sum_reward)
    
  # print(f"Resulting Q Table Sarsa: \n{q_tab}\n")
  environment = np.zeros((4,12))
  environment[3,0], environment[3,11] = 1,1
  for p in path:
    first,second = p[0],p[1]
    environment[first,second] = 1
  #print(f"Final Traversal ({final_iter_steps} steps): \n{environment}")
  return all_rewards

#sars_learn = SarsaPlay(Agent())

"""#### Q-Learning Implementation"""

def Qplay(agent, episodes = 500, gamma = 1, alpha = 0.1, epsilon = 0.1):
  qq_tab = np.zeros((4, 4 * 12))
  qall_rewards = []
  qpath = []
  qfinal_iter_steps = 0
  for i in range(episodes):
    pos = agent.start
    finished = False
    sum_reward = 0
    while finished == False:
      #state, action, move
      state = 12 * pos[0] + pos[1]
      #getting action using epsilon-greedy
      action = agent.chooseAction(state, qq_tab, epsilon)

      #move agent - call environment move method passing in agent coords and action number
      #print(f"Agent position: {pos}, about to do action: {action}")
      pos = agent.env.move(pos, action)
      #print(f"Agent new position: {pos}, done action: {action}")

      
      #get next state
      next_state = 12 * pos[0] + pos[1]

      #get reward
      reward = agent.env.reward(pos)
      sum_reward += reward
      
      #print(agent.env.reward_table)
      #finished = True

      #see if game ends - check value of reward retrieved from previous method call
      if reward == -100:
        #print("done - cliff")
        finished = True
      elif reward == 0:
        #print("done - end")
        finished = True
      
      #if finished, update with 0 next state val, else, normal q update and action select
      if finished == True:
        qq_tab[action][state] = qq_tab[action][state] + alpha * (reward + (gamma * 0.0) - qq_tab[action][state])
        break
      else:
        #choose next action using greedy selection and state (Difference between Sarsa and Q)
        next_action = np.argmax(qq_tab[:,state])

        #update q table
        next_state_val = qq_tab[next_action][next_state]
        qq_tab[action][state] = qq_tab[action][state] + alpha * (reward + (gamma * next_state_val) - qq_tab[action][state])

      #choose next action using policy and next state
      #next_action = agent.chooseAction(next_state, qq_tab)

      #update q table
      #next_state_val = qq_tab[next_action][next_state]
      #qq_tab[action][state] = qq_tab[action][state] + alpha * (reward + (gamma * next_state_val) - qq_tab[action][state])

      #update state - only the state in q learning
      state = next_state
      
      if i == episodes - 1:
        qpath.append(pos)
        qfinal_iter_steps += 1
    qall_rewards.append(sum_reward)
    
  # print(qq_tab)
  qenvironment = np.zeros((4,12))
  qenvironment[3,0], qenvironment[3,11] = 1,1
  for p in qpath:
    first,second = p[0],p[1]
    qenvironment[first,second] = 1
  #print(f"Final Traversal ({qfinal_iter_steps} steps): \n{qenvironment}")
  return qall_rewards

#qlearn = Qplay(Agent())

"""#### Plotting Rewards"""

def plotGraphs(epsilons):
  for eps in epsilons:
    #Initial list comp to obtain ten runs of each RL method 
    sAvgList = [SarsaPlay(Agent(), alpha = 0.1, epsilon = eps) for i in range(10)]
    qAvgList = [Qplay(Agent(), alpha = 0.1, epsilon = eps) for i in range(10)]
    #List comp average for ten runs of each RL method
    sarsaAvg = [(sAvgList[0][i] + sAvgList[1][i] + sAvgList[2][i] + sAvgList[3][i] + sAvgList[4][i] + sAvgList[5][i] + sAvgList[6][i] + sAvgList[7][i] + sAvgList[8][i] + sAvgList[9][i])/10 for i in range(500)]
    qplayAvg = [(qAvgList[0][i] + qAvgList[1][i] + qAvgList[2][i] + qAvgList[3][i] + qAvgList[4][i] + qAvgList[5][i] + qAvgList[6][i] + qAvgList[7][i] + qAvgList[8][i] + qAvgList[9][i])/10 for i in range(500)]
    #plot average rewards
    plt.plot(sarsaAvg, label = "Sarsa")
    plt.plot(qplayAvg, label = 'QLearn')
    plt.legend()
    plt.title(f"Rewards average, epsilon = {eps}, SARSA vs QLearning")
    plt.show()

#Plotting graphs of averages using list of epsilon values
plotGraphs([0.1, 0.2, 0.3, 0.5, 1])